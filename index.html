<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<meta name="keywords" content="Tao Shen, Shen Tao, SMBU, Economics">
	<meta name="description" content="Tao Shen's Home">
	<link rel="stylesheet" href="jemdoc.css" type="text/css">

	<link rel="preconnect" href="https://fonts.loli.net">
	<link rel="preconnect" href="https://gstatic.loli.net" crossorigin>
	<link href="https://fonts.loli.net/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300&display=swap"
		rel="stylesheet">

	<title>me@shentao</title>

	<style>
		/* Á≠æÂêçÂõæÁâáÁöÑÂ§ßÂ∞èÊéßÂà∂ */
		.sign-img {
			width: 120px;
			margin-top: 5px;
		}
	</style>

</head>

<body>
	<div id="layout-content" style="margin-top:25px">

		<a href="https://github.com/shentao-coder" class="github-corner"><svg width="80" height="80"
				viewBox="0 0 250 250"
				style="fill:#527bbd; color:#fff; position: absolute; top: 0; border: 0; right: 0;">
				<path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
				<path
					d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
					fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
				<path
					d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
					fill="currentColor" class="octo-body"></path>
			</svg></a>
		<style>
			.github-corner:hover .octo-arm {
				animation: octocat-wave 560ms ease-in-out
			}

			@keyframes octocat-wave {

				0%,
				100% {
					transform: rotate(0)
				}

				20%,
				60% {
					transform: rotate(-25deg)
				}

				40%,
				80% {
					transform: rotate(10deg)
				}
			}

			@media (max-width:500px) {
				.github-corner:hover .octo-arm {
					animation: none
				}

				.github-corner .octo-arm {
					animation: octocat-wave 560ms ease-in-out
				}
			}
		</style>

		<table>
			<tbody>
				<tr>
					<td width="670">
						<div id="toptitle">
							<h1>Tao Shen </h1>
							<img src="pic/sign.png" border="0" width="120" alt="Signature">
						</div>

						<h3>üéì Prospective PhD Candidate</h3>
						<p>
							Shenzhen MSU-BIT University<br>
							Shenzhen, China 518172.<br>
							Telephone: +86 185-2908-5521<br>
							Email: shentao.mr [at] gmail.com<br>
						</p>

						<p style="margin-top: 15px;">
							<a href="https://www.facebook.com/share/1CYyi7rwaD/?mibextid=wwXIfr" target="_blank"
								title="Facebook">
								<img src="pic/facebook.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="Facebook">
							</a>

							<a href="https://www.linkedin.com/in/taoshen-work" target="_blank" title="LinkedIn">
								<img src="pic/linkedin.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="LinkedIn">
							</a>

							<a href="https://x.com/taoshentaoshen?s=21" target="_blank" title="X">
								<img src="pic/x.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="X">
							</a>

							<a href="pic/douyinqr.jpg" target="_blank" title="Douyin">
								<img src="pic/douyin.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="Douyin">
							</a>

							<a href="pic/wechatqr.jpg" target="_blank" title="WeChat">
								<img src="pic/wechat.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="WeChat">
							</a>

							<a href="https://www.instagram.com/shent.ao?igsh=Ym10MW9rOW82dTU2&utm_source=qr"
								target="_blank" title="Instagram">
								<img src="pic/instagram.png" height="30px" style="margin-right:10px; margin-bottom:-3px"
									alt="Instagram">
							</a>

							<a href="https://xhslink.com/m/1NKC2xXHesO" target="_blank" title="Rednote">
								<img src="pic/xiaohongshu.png" height="30px"
									style="margin-right:10px; margin-bottom:-3px" alt="Xiaohongshu">
							</a>
						</p>
					</td>
					<td>
						<img src="pic/me.png" border="0" width="240" style="border-radius:5px" alt="Tao Shen"><br>
					</td>
				</tr>
			</tbody>
		</table>

		<h2>Biography</h2>
		<p>
			Hello! I am <b>Tao Shen</b>. I obtained my <b>Bachelor of Economics</b> in International Economics and Trade
			from <a href="https://en.smbu.edu.cn/" target="_blank">Shenzhen MSU-BIT University (SMBU)</a> in 2025.
			My academic background is rooted in a rigorous curriculum jointly established by <a
				href="https://english.bit.edu.cn/" target="_blank">Beijing Institute of Technology (BIT)</a> and <a
				href="https://www.msu.ru/en/" target="_blank">Lomonosov Moscow State University (MSU)</a>.
		</p>
		<p>
			I have gained extensive industry experience through internships at <b>CITIC Futures</b>, <b>China CITIC Bank
				International</b>, and <b>Agricultural Bank of China</b>.
			These experiences have sharpened my skills in data analysis (Python/Stata), market research, and operational
			management.
		</p>

		<p>
			My research interests lie in <b>Applied Econometrics</b>, <b>Financial Markets</b>, and <b>Data Science</b>.
			I am dedicated to bridging the gap between economic theory and real-world business problems through
			quantitative methods.
		</p>

		<p>
			<i style="color: #527bbd;">Feel free to contact me via email for potential research collaborations or job
				opportunities.</i>
		</p>


		<h2>News</h2>
		<div style="height: 200px; overflow: auto;">
			<ul>
				<li>
					[09/2025] One paper are accepted by <a href="https://link.springer.com/journal/11263">IJCV 2025</a>
					!
				</li>
				<li>
					[04/2025] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ACM MM
						2025</a> !
				</li>
				<li>
					[03/2025] Two paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ICCV
						2025</a> !
				</li>
				<li>
					[01/2025] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">CVPR
						2025</a> !
				</li>
				<li>
					[12/2024] Two paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ICLR
						2025</a> !
				</li>
				<li>
					[12/2024] <i style="color: red; display: inline;">Five</i> paper are accepted by <a
						href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a> !
				</li>
				<li>
					[09/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">WACV 2025</a> !
				</li>
				<li>
					[08/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">NeurIPS 2024</a> !
				</li>
				<li>
					[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">Siggraph Asia
						2024</a> !
				</li>
				<li>
					[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">ACM MM 2024</a> !
				</li>
				<li>
					[06/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Emoji</i>, a
					freestyle portrait animation framework. Enjoy <a href="https://github.com/mayuelala">
						FollowYourEmoji</a> quickly !
				</li>
				<li>
					[03/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Click</i>, the
					first framework to achieve regional image animation. Enjoy the fun of <a
						href="https://github.com/mayuelala/FollowYourClick">FollowYourClick</a> !
				</li>
				<li>
					[01/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">CVPR 2024</a> !
				</li>
				<li>
					[12/2023] Two paper are accepted by <a href="https://aaai.org/aaai-conference/">AAAI 2024</a> !
				</li>
				<li>
					[11/2023] Excited to release <i style="color: red; display: inline;">MagicStick</i>, the first
					unified framework to modify video properties leveraging the keyframe transformations on the
					extracted internal control signals.
				</li>
				<li>
					[10/2023] One paper is accepted by <a
						href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM 2024</a> !
				</li>
				<li>
					[09/2023] I am serving as a Reviewer for <a href="https://iclr.cc/">ICLR 2024</a> !
				</li>
				<li>
					[07/2023] I start to cooperate with <a
						href="https://scholar.google.com/citations?user=lLMX9hcAAAAJ">Qifeng Chen</a> closely in <a
						href="https://cse.hkust.edu.hk/">HKUST</a> !
				</li>
				<li>
					[06/2023] I end my research intership in <a href="https://ai.tencent.com/">Tencent AI Lab</a> !
				</li>
				<li>
					[06/2023] I am serving as a Reviewer for <a href="https://nips.cc/">NeurIPS 2023</a> !
				</li>
				<li>
					[05/2023] I am serving as a Reviewer for <a href="https://www.acmmm2023.org/">ACM MM 2023</a> !
				</li>
				<li>
					[04/2023] Excited to release <i style="color: red; display: inline;">Follow-Your-Pose</i> framework,
					generating the character videos from pose and text description. Enjoy the fun of <a
						href="https://github.com/mayuelala/FollowYourPose">FollowYourPose</a> !
				</li>
				<li>
					[03/2023] Two paper are submitted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> !
				</li>
				<li>
					[02/2023] One paper <a herf="https://arxiv.org/abs/2302.05940">SemanticAC</a> about Audio
					Classification was accepted by <a herf="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
				</li>
				<li>
					[12/2022] Awarded First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua
						Shenzhen International Graduate School</a> !
				</li>
				<li>
					[11/2022] One paper about video-text pretraining with masked autoencoder is submitted to <a
						href="https://cvpr2023.thecvf.com/CVPR">CVPR 2023</a> !
				</li>
				<li>
					[10/2022] One paper about semantics-assisted framework for audio classification is submitted to <a
						href="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
				</li>
				<li>
					[09/2022] Attending <a href="https://2022.acmmm.org/">ACM MM 2022</a>, Welcome to chat!
				</li>
				<li>
					[09/2022] <a href="https://">My first paper</a> has been accepted for <i
						style="color: red; display: inline;">Oral Presentation</i> on ACM MM 2022 (Top 5%) !
				</li>
				<li>
					[05/2022] Joined <a href="https://ai.tencent.com/">Tencent AI Lab</a> as research intern !
				</li>
				<li>
					[03/2022] Awarded <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research
						Elite Program</a> from Tencent ! Only <i style="color: red; display: inline;">72</i> students in
					the world admitted to this program !
				</li>
				<li>
					[07/2021] Joined <a href="http://mmlab.siat.ac.cn/">CUHK@MMLab in ShenZhen</a> as research intern !
				</li>
				<li>
					[06/2021] Recommended to <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> towards the
					MSc degree !
				</li>
				<li>
					[06/2021] Graduated from <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a> !
				</li>
				<li>
					[01/2021] Joined <a href="https://www.tsinghua.edu.cn/">Baidu Research</a> as research intern.
					Started doing research on Computer Vision !
				</li>
			</ul>
		</div>

		<h2>Industry Experience</h2>
		<table id="tbPublications" width="100%">
			<tbody>
				<tr>
					<td width="200">
						<img src="pic/citicf.png" width="160px">
					</td>
					<td>
						<p><b>CITIC Futures Co., Ltd.</b></p>
						<p>Intern, International Business Center</p>
						<p><b>2025.02 - 2025.05 | Futian District, Shenzhen, China</b></p>
						<p style="font-size: 0.95em; color: #555;">
							Leveraged <b>Wind & Python</b> to mine data for 2000+ potential institutional clients;
							Created <b>CLSA-standard</b> pitch decks to visualize complex market data.
						</p>
					</td>
				</tr>

				<tr>&nbsp;</tr>
				<tr>
					<td width="200">
						<img src="pic/cncbi.png" width="160px">
					</td>
					<td>
						<p><b>China CITIC Bank International</b></p>
						<p>Intern, Operation Management</p>
						<p><b>2024.11 - 2024.12 | Futian District, Shenzhen, China</b></p>
						<p style="font-size: 0.95em; color: #555;">
							Conducted competitor analysis on <b>mortgage policies</b>;
							Ensured <b>100% accuracy</b> in daily financial data reporting to the PBOC.
						</p>
					</td>
				</tr>

				<tr>&nbsp;</tr>

				<tr>
					<td width="200">
						<img src="pic/abc.png" width="160px">
					</td>
					<td>
						<p><b>Agricultural Bank of China</b></p>
						<p>Intern, Customer Manager Assistant</p>
						<p><b>2024.07 - 2024.08 | Longgang District, Shenzhen, China</b></p>
						<p style="font-size: 0.95em; color: #555;">
							Managed daily operations for <b>200+ customers</b> with zero complaints;
							Proposed and implemented optimization for <b>elderly service processes</b>.
						</p>
					</td>
				</tr>
			</tbody>
		</table>


		<!-- The University of Sydney, Australia

Doctor of Philosophy (Ph.D.) Candidate in Computer Science
Team: Multimedia Laboratory, Sydney (MMLab@Sydney)
 Prof. Wanli Ouyang, Prof. Chang Xu
2022 - Present -->



		<h2>Education & Exchange</h2>
		<table id="tbPublications" width="100%">
			<tbody>
				<tr>
					<td width="200">
						<img src="pic/smbu.png" width="160px">
					</td>
					<td>
						<p><b>Shenzhen MSU-BIT University</b></p>
						<p>B.Econ. in International Economics and Trade</p>
						<p><b>2021.08 - 2025.06 | Shenzhen, China</b></p>
						<p style="font-size: 0.95em; color: #555; margin-top: 5px;">
							<b>GPA: 3.74 / 4.00</b>
							 
							University Entrance Scholarship<br>
							Full English Instruction & Global Faculty<br>
							Rigorous Quantitative & Economic Theory Training
						</p>
					</td>
				</tr>

				<tr>&nbsp;</tr>
				<tr>
					<td width="200">
						<img src="pic/mq.svg" width="160px">
					</td>
					<td>
						<p><b>Macquarie University</b></p>
						<p>International Visiting Student (Short-term Program)</p>
						<p><b>2024.01 - 2024.02 | Sydney, Australia</b></p>
						<p style="font-size: 0.95em; color: #555;">
							Completed an intensive cross-cultural business project and awarded <b>Program
								Certificate</b>;<br>
							Conducted field research on local enterprises facilitated by the <b>Australia Fujian Chamber
								of Commerce</b>.
						</p>
					</td>
				</tr>
			</tbody>
		</table>


		<h2> Selected Publications | <a href="https://scholar.google.com/citations?user=kwBR1ygAAAAJ&hl=zh-CN">Full
				List</a></h2>
		<!--
<div style="height: 1440px; overflow: auto;">
-->
		<table id="tbPublications" width="100%">
			<tbody>
				<td><b>/*Preprints*/</b>
				</td>
				<tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>
					<td width="306">
						<img src="./indexpics/2025-survey.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>üéâ Controllable Video Generation: A Survey</b></p>
						<p><b>Yue Ma</b>, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He,
							Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu,
							Linfeng Zhang, Qifeng Chen</p>
						<em>arXiv preprint:2507.16869. 2025</em>
						<p> [<a href="https://arxiv.org/pdf/2507.16869">paper</a>] [<a
								href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation">code</a>] </p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>
					<td width="306">
						<img src="./indexpics/2025-nips.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>üéâ Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</b></p>
						<p><b>Yue Ma</b>, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan
							Zhang, Ayden Yang, Zeyu Wang, Qifeng Chen</p>
						<em>arXiv preprint:2506.04590. 2025</em>
						<p> [<a href="https://arxiv.org/pdf/2506.04590">paper</a>] [<a
								href="https://follow-your-creation.github.io/">code</a>] </p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>
					<td width="306">
						<img src="./indexpics/2025-motion.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>üéâ Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled
								Finetuning</b></p>
						<p><b>Yue Ma</b>, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li,
							Sirui Han, Chenyang Qi, Qifeng Chen</p>
						<em>arXiv preprint:2506.05207. 2025</em>
						<p> [<a href="https://arxiv.org/pdf/2506.05207">paper</a>] [<a
								href="https://follow-your-motion.github.io/">code</a>] </p>
					</td>
				</tr>
				<!--########################-->
				<!-- <tr>
		<td width="306">
		<img src="./indexpics/2022-simvtp-framework.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</b></p>
		<p><b>Yue Ma</b>, Tianyu Yang, Ying Shan, Xiu Li</p>
		<em>arXiv preprint:2211.03490. 2022</em>
		<p> [<a href="https://arxiv.org/pdf/2211.03490">paper</a>] [<a href="https://github.com/mayuelala/SimVTP">code</a>] </p>
		</td>
	</tr> -->
				<!--########################-->
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<td><b>/*Journal*/</b>

					<tr>
						<td width="306">
							<img src="./indexpics/2025-IJCV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td>
							<p><b>Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive
									Freestyle Portrait Animation</b></p>
							<p><b>Yue Ma</b>, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan,
								Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang, Qifeng
								Chen</p>
							<em>International Journal of Computer Vision(<b>IJCV</b>), 2025</em>
							<p> [<a href="https://mayuelala.github.io/">paper</a>] [<a
									href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a
									href="https://follow-your-click.github.io/">project page</a>]
							</p>
						</td>
					</tr>
				<td><b>/*Conference*/</b>
					<p></p>
				</td>
				<tr>
					<td width="306">
						<img src="./indexpics/2024-followyourclick.png" width="285px"
							style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</b></p>
						<p><b>Yue Ma</b>, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li,
							Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
						<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
						<p> [<a href="https://arxiv.org/abs/2403.08268">paper</a>] [<a
								href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a
								href="https://follow-your-click.github.io/">project page</a>]
						</p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>
					<td width="306">
						<img src="./indexpics/2024-Canvas.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content
								Generation</b></p>
						<p>Qihua Chen*, <b>Yue Ma*</b>, Hongfa Wang*, Junkun Yuan*, Wenzhe Zhao, Qi Tian, Hongmei Wang,
							Shaobo Min, Qifeng Chen, Wei Liu</p>
						<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
						<p> [<a href="hhttps://arxiv.org/abs/2409.01055">paper</a>] [<a
								href="https://github.com/mayuelala">code</a>] [<a
								href="https://github.com/mayuelala">project page</a>]
						</p>
					</td>
				</tr>
				<tr>&nbsp</tr>


				<!--########################-->
				<tr>
					<td width="306">
						<img src="./indexpics/2024-Fypv2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose
								Control</b></p>
						<p>Jingyun Xue, Hongfa Wang, Qi Tian, <b>Yue Ma</b>, Andong Wang, Zhiyuan Zhao, Shaobo Min,
							Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo</p>
						<em>International Conference on Learning Representations(<b>ICLR</b>) 2025</em>
						<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a
								href="https://github.com/mayuelala">code</a>] [<a
								href="https://github.com/mayuelala">project page</a>]
						</p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<!--########################-->
				<tr></tr>
				<td width="306">
					<img src="./indexpics/2024-followyouremoji.png" width="285px" style="box-shadow: 4px 4px 8px #888">
				</td>
				<td>
					<p><b>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></p>
					<p><b>Yue Ma</b>, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei
						Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
					<em>The ACM Special Interest Group for Computer Graphics and Interactive Techniques(<b>Siggraph
							Asia</b>) 2024</em>
					<p> [<a href="https://arxiv.org/abs/2406.01900">paper</a>] [<a
							href="https://github.com/mayuelala">code</a>] [<a
							href="https://follow-your-emoji.github.io/">project page</a>]
					</p>
				</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>
					<td width="306">
						<img src="./indexpics/2023-MagicStick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>Follow-Your-Handle: Controllable Video Editing via Control Handle Transformations</b></p>
						<p><b>Yue Ma</b>, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng
							Chen</p>
						<em>IEEE /CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025</em>
						<p> [<a href="https://arxiv.org/abs/2312.03047">paper</a>] [<a
								href="https://github.com/mayuelala/MagicStick">code</a>] [<a
								href="https://magic-stick-edit.github.io/">project page</a>]
						</p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<!--########################-->
				<tr>
					<td width="306">
						<img src="./indexpics/2023-iccv-followyourpose.png" width="285px"
							style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>üï∫üï∫üï∫ Follow-Your-Pose üíÉüíÉüíÉ: Pose-Guided Text-to-Video Generation using Pose-Free
								Videos</b></p>
						<p><b>Yue Ma</b>, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng
							Chen</p>
						<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
						<p><a href="https://www.paperdigest.org/2024/09/most-influential-aaai-papers-2024-09/"
								style="color: red; display: inline;"><b>PaperDigest Most Influential Papers of AAAI
									24</b></a></p>
						<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a
								href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a
								href="https://follow-your-pose.github.io/">project page</a>]
							<a target="_blank" href="https://github.com/mayuelala/FollowYourPose"><img
									alt="GitHub stars" align="right"
									src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social"></a><a></a>
						</p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<!--########################-->
				<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-AAAI-MBEV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>M-BEV: Masked BEV Perception for Robust Autonomous Driving</b></p>
		<p>Siran Chen, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> -->
				<!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-icassp-audio.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SemanticAC: Semantics-Assisted Framework for Audio Classification</b></p>
		<p>Yicheng Xiao*, <b>Yue Ma*</b>, Shuyan Li, Hantao Zhou, Ran Liao, Xiu Li (* equal contribution)</p>
		<em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023. </em>
		<i></i>
		<p> [<a href="https://arxiv.org/abs/2302.05940">paper</a>] [<a href="https://github.com/mayuelala">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
				<tr>
					<td width="306">
						<img src="./indexpics/2022-mm-graph.png" width="285px" style="box-shadow: 4px 4px 8px #888">
					</td>
					<td>
						<p><b>Visual Knowledge Graph for Human Action Reasoning in Videos</b></p>
						<p><b>Yue Ma</b>, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, Yu Qiao</p>
						<em>The 30th ACM International Conference on Multimedia. (<b>ACM MM</b>), 2022. </em>
						<i>
							<p style="color: red; display: inline;">(<b>Oral Presentation</b>)</p>
						</i>
						<p> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548257">paper</a>] [<a
								href="https://github.com/mayuelala/AKU">code</a>] </p>
					</td>
				</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>

				<!-- <td><b>/*Journal*/</b>
		<p></p>
		</td>
		
		<tr>
			<td width="306">
			<img src="./indexpics/2023-TMM-mmlab.png" width="285px" style="box-shadow: 4px 4px 8px #888">
			</td>				
			<td>
			<p><b>Attentive Snippet Prompting for Video Retrieval</b></p>
			<p>Siran Chen, Qinglin Xu, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
			<em>IEEE Transactions on Multimedia (<b>TMM</b>), 2024. </em>
			<i></i>
			<p> [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">paper</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">code</a>] </p>
			</td>
		</tr> -->
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>
				<tr>&nbsp</tr>


			</tbody>
		</table>
		<!--
</div>
-->




		<h2>Honors &amp; Awards</h2>
		<table style="border-spacing:2px">
			<tbody>
				<tr>
					<td> [06/2024] Outstanding graduates student of Beijing.</td>
				</tr>
				<tr>
					<td> [08/2023] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua
							University</a>.</td>
				</tr>
				<tr>
					<td> [12/2022] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">SIGS</a>, Tsinghua
						University.</td>
				</tr>
				<tr>
					<td> [03/2022] <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite
							Program</a>, only 72 students in the world admitted to this program.</td>
				</tr>
				<tr>
					<td> [09/2020] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan
							University of Technology</a>.</td>
				</tr>.
				<tr>
					<td> [06/2019] Excellent Scientific Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University
							of Technology</a>.</td>
				</tr>.
				<tr>
					<td> [09/2019] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan
							University of Technology</a>.</td>
				</tr>
				<tr>
					<td> [09/2018] Excellent Academic Progress Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan
							University of Technology</a>.</td>
				</tr>.
				<tr>
					<td> [06/2018] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan
							University of Technology</a>.</td>
				</tr>.

			</tbody>
		</table>


		<h2>Professional Services</h2>
		<ul>
			<li>
				<b>Student Reviewers:</b><br>
				Computer Vision and Pattern Recognition (CVPR)<br>
				International Conference on Computer Vision (ICCV)<br>
				European Conference on Computer Vision (ECCV)<br>
				Conference and Workshop on Neural Information Processing Systems (NeurIPS)<br>
				International Conference on Learning Representations (ICLR) <br>
				AAAI Conference on Artificial Intelligence (AAAI)<br>
				IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
			</li>


		</ul>


		<h2>Teaching</h2>
		<table id="tbTeaching" border="0" width="100%">
			<tbody>
				<tr>
					<td> 2022-2023</td>
					<td>Fall</td>
					<td>Artificial Intelligence Technology (THU, 85990263-200)</td>
				</tr>
			</tbody>
		</table>




		<div id="footer">
			<div id="footer-text"></div>
		</div>
		<script type='text/javascript' id='clustrmaps'
			src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>
		<p>
			<center> &copy; Tao Shen </center>
		</p>


	</div>
</body>

</html>